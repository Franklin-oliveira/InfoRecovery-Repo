{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução à Recuperação de Informações\n",
    "\n",
    "----\n",
    "**Aluno:** Franklin Alves de Oliveira\n",
    "\n",
    "----\n",
    "\n",
    "## Lista de exercícios 1\n",
    "\n",
    "Todos os exercícios requerem uma implementação funcional do problema, no corpo do notebook. Para cada exercício consultas de teste devem ser propostas para demonstrar que a implementação atende aos requisitos do exercício.\n",
    "As resposta devem ser enviadas como um notebook (.ipynb) para o professor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Antes de começar o exercício, vamos importar algumas bibliotecas e o corpus com o qual vamos trabalhar...</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# métodos do nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer as WPT\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# instanciando o Stemmer\n",
    "Stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Corpus escolhido:** Para a realização desse exercício, vamos utilizar um corpus composto por *reviews* de produtos eletrônicos do site [Amazon.com](www.amazon.com). \n",
    "\n",
    "Esse corpus, e muitos outros semelhantes, podem ser encontrados nesse [link](http://jmcauley.ucsd.edu/data/amazon/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AO94DHGC771SJ</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>amazdnu</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>We got this GPS for my husband who is an (OTR)...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Gotta have GPS!</td>\n",
       "      <td>1370131200</td>\n",
       "      <td>06 2, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AMO214LNFCEI4</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>[12, 15]</td>\n",
       "      <td>I'm a professional OTR truck driver, and I bou...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Very Disappointed</td>\n",
       "      <td>1290643200</td>\n",
       "      <td>11 25, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A3N7T0DY83Y4IG</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>C. A. Freeman</td>\n",
       "      <td>[43, 45]</td>\n",
       "      <td>Well, what can I say.  I've had this unit in m...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1st impression</td>\n",
       "      <td>1283990400</td>\n",
       "      <td>09 9, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A1H8PY3QHMQQA0</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>Dave M. Shaw \"mack dave\"</td>\n",
       "      <td>[9, 10]</td>\n",
       "      <td>Not going to write a long review, even thought...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Great grafics, POOR GPS</td>\n",
       "      <td>1290556800</td>\n",
       "      <td>11 24, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>A24EV6RXELQZ63</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>Wayne Smith</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I've had mine for a year and here's what we go...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Major issues, only excuses for support</td>\n",
       "      <td>1317254400</td>\n",
       "      <td>09 29, 2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin              reviewerName   helpful  \\\n",
       "0   AO94DHGC771SJ  0528881469                   amazdnu    [0, 0]   \n",
       "1   AMO214LNFCEI4  0528881469           Amazon Customer  [12, 15]   \n",
       "2  A3N7T0DY83Y4IG  0528881469             C. A. Freeman  [43, 45]   \n",
       "3  A1H8PY3QHMQQA0  0528881469  Dave M. Shaw \"mack dave\"   [9, 10]   \n",
       "4  A24EV6RXELQZ63  0528881469               Wayne Smith    [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  We got this GPS for my husband who is an (OTR)...      5.0   \n",
       "1  I'm a professional OTR truck driver, and I bou...      1.0   \n",
       "2  Well, what can I say.  I've had this unit in m...      3.0   \n",
       "3  Not going to write a long review, even thought...      2.0   \n",
       "4  I've had mine for a year and here's what we go...      1.0   \n",
       "\n",
       "                                  summary  unixReviewTime   reviewTime  \n",
       "0                         Gotta have GPS!      1370131200   06 2, 2013  \n",
       "1                       Very Disappointed      1290643200  11 25, 2010  \n",
       "2                          1st impression      1283990400   09 9, 2010  \n",
       "3                 Great grafics, POOR GPS      1290556800  11 24, 2010  \n",
       "4  Major issues, only excuses for support      1317254400  09 29, 2011  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importanto o corpus (selecionando apenas mil observações)\n",
    "corpus = pd.read_csv('../Amazon_electronics_reviews.csv', sep=',', encoding='utf-8', \n",
    "                                                          index_col=0, nrows=10000)\n",
    "# removendo linhas sem reviews\n",
    "corpus = corpus.dropna(subset=['reviewText'])\n",
    "\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1689188, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1689188 entries, 0 to 1689187\n",
      "Data columns (total 9 columns):\n",
      "reviewerID        1689188 non-null object\n",
      "asin              1689188 non-null object\n",
      "reviewerName      1664309 non-null object\n",
      "helpful           1689188 non-null object\n",
      "reviewText        1688117 non-null object\n",
      "overall           1689188 non-null float64\n",
      "summary           1689173 non-null object\n",
      "unixReviewTime    1689188 non-null int64\n",
      "reviewTime        1689188 non-null object\n",
      "dtypes: float64(1), int64(1), object(7)\n",
      "memory usage: 128.9+ MB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que, do dataframe **`corpus`**, nossa coluna de interesse nesse exercício é a `reviewText`, que contém exatamente os textos de avaliação de produtos eletrônicos escritos pelos usuários."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercício 1: Truncagem e revocação.\n",
    "\n",
    "Baseando-se no indice invertido construído na prática 1, Calcule a diferença de revocação com e sem a utilização de \"stemming\", ou truncagem na construção do índice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>Solução:</font>\n",
    "\n",
    "**1. Vamos começar preparando o corpus para a criação do índice invertido.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "caracteres especiais: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# preparando lista de stopwords (em inglês)\n",
    "swu = stopwords.words('english') + list(string.punctuation)\n",
    "\n",
    "# note que também vamos remover caracteres especiais do texto\n",
    "print('stopwords:', swu[:10])\n",
    "print('caracteres especiais:', string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Em seguida, vamos tokenizar os textos (*reviews*). A técnica de *stemming* consiste em reduzir o termo ao seu radical, removendo afixos e vogais temáticas. A tabela a seguir exibe alguns exemplos de palavras \"stemizadas\": \n",
    "\n",
    "| token  | stem |\n",
    "|---|---|\n",
    "| impressed  | impress |\n",
    "| professional  | profession  | \n",
    "| hoing  | hope | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizando os textos\n",
    "corpus['tokenized'] = corpus['reviewText'].apply(lambda x:WPT().tokenize(str(x).lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removendo stopwords\n",
    "corpus['tokenized'] = corpus['tokenized'].apply(lambda w:[i for i in w if i not in swu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos criar uma coluna com o texto \"stemizado\"\n",
    "corpus['stemmed'] = corpus['tokenized'].apply(lambda y:[Stemmer.stem(j) for j in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['professional', 'otr', 'truck', 'driver', 'bought', 'tnd', '700', 'truck', 'stop', 'hoping']\n",
      "Stems: ['profession', 'otr', 'truck', 'driver', 'bought', 'tnd', '700', 'truck', 'stop', 'hope']\n"
     ]
    }
   ],
   "source": [
    "print('Tokens:', corpus['tokenized'].iloc[1][:10])\n",
    "print('Stems:', corpus['stemmed'].iloc[1][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**OBS:** Na coluna `tokenized`, temos apenas o texto tokenizado e sem *stopwords*. Já na coluna `stemmed`, além de tokenizado e removidas as *stopwords*, também \"stemizamos\" o texto.\n",
    "\n",
    "----\n",
    "\n",
    "**2. Agora, vamos criar um índice invertido para as colunas `tokenized` e `stemmed`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_index(lista_textos):\n",
    "    '''\n",
    "    Cria um índice invertido para uma lista de textos (tokenizados). \n",
    "    Retorna o ID do documento em que a palavra foi encontrada.\n",
    "    '''\n",
    "    indice = defaultdict(lambda:set([]))\n",
    "    \n",
    "    for i, texto in enumerate(lista_textos):\n",
    "        for term in texto:\n",
    "            indice[term].add(i)\n",
    "\n",
    "    return indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice_token = inv_index(corpus['tokenized'].values)\n",
    "indice_stem = inv_index(corpus['stemmed'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{256, 1, 355, 580, 710, 742, 171, 363}\n",
      "{256, 1, 355, 580, 710, 742, 171, 363}\n"
     ]
    }
   ],
   "source": [
    "print(indice_token['professional'])              # busca no texto tokenizado\n",
    "print(indice_stem[Stemmer.stem('professional')]) # busca no texto \"stemizado\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**3. Antes de continuarmos, vamos criar uma função de busca**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, index, kind_index='token'):\n",
    "\n",
    "    # tokenizando a busca\n",
    "    tokens = WPT().tokenize(query.lower())\n",
    "    \n",
    "    if kind_index == 'stem':\n",
    "        tokens = [Stemmer.stem(w) for w in tokens]\n",
    "\n",
    "    # criando um conjunto para armazenar o resultado (conj. para eliminar duplicados)\n",
    "    result = set()\n",
    "    \n",
    "    # definindo o tipo de operador (or, and or not)\n",
    "    operator = 'and'\n",
    "    if 'and' in tokens:\n",
    "        operator = 'and'\n",
    "    elif 'or' in tokens:\n",
    "        operator = 'or'\n",
    "    elif 'not' in tokens:\n",
    "        operator = 'not'\n",
    "    \n",
    "    # faz a consulta (query)\n",
    "    for token in tokens:\n",
    "        if token == 'and' or token == 'or' or token == 'not':\n",
    "            continue\n",
    "            \n",
    "        temp = index[token]\n",
    "        \n",
    "        if operator == 'or':\n",
    "            result = result | temp\n",
    "        elif operator == 'not':\n",
    "            result = result - temp\n",
    "        elif operator == 'and':\n",
    "            result = result | temp\n",
    "        else:\n",
    "            print('operador não definido')\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 171, 256, 291, 355, 363, 387, 438, 488, 529, 580, 710, 713, 734, 742, 897}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('professional and hoping', indice_token, kind_index='token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "Agora, vamos analisar as taxas de **precisão**(*precision*) e **revocação** (*recall*) baseadas no índice invertido que acabamos de criar. A taxa de precisão pode ser calculada conforme a fórmula abaixo:\n",
    "\n",
    "$$Precisão = \\frac{\\#\\{ \\text{documentos recuperados} \\cap \\text{documentos relevantes} \\}}{\\#\\{ \\text{documentos recuperados}\\}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Que é exatamente a taxa de resultados corretos em relação aos resultados retornados. Já a taxa de revocação, é definida como \n",
    "\n",
    "<br>\n",
    "\n",
    "$$Revocação = \\frac{\\#\\{ \\text{documentos recuperados} \\cap \\text{documentos relevantes} \\}}{\\#\\{ \\text{documentos relevantes}\\}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "que é o o número de resultados corretos dividido pelo número de resultados que deveriam ter sido retornados. \n",
    "\n",
    "----\n",
    "\n",
    "Com isso em mente, \n",
    "\n",
    "**4. Vamos definir funções para calcular as taxas de precisão e revocação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precisao(recuperados, relevantes):\n",
    "    return len(recuperados & relevantes)/len(recuperados)\n",
    "\n",
    "def revocacao(recuperados, relevantes):\n",
    "    return len(recuperados &  relevantes)/len(relevantes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Comparando precisão e revocação para os textos com e sem _stemming_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Resultados Não-Stemmizados:  16\n",
      "Quantidade de Resultados Stemmizados: 41\n"
     ]
    }
   ],
   "source": [
    "# Texto tokenizado (sem stemming)\n",
    "expression = 'professional and hoping'\n",
    "\n",
    "# Resultados da busca\n",
    "res_normal = search(expression,indice_token, kind_index='token')\n",
    "res_stem = search(expression, indice_stem, kind_index='stem')\n",
    "\n",
    "print('Quantidade de Resultados Não-Stemmizados: ', len(res_normal))\n",
    "print('Quantidade de Resultados Stemmizados:', len(res_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisão:  1.0\n",
      "Revocação:  0.3902439024390244\n",
      "\n",
      "Diferença: 0.6097560975609756\n"
     ]
    }
   ],
   "source": [
    "prec = precisao(res_normal, res_stem)\n",
    "rev = revocacao(res_normal, res_stem)\n",
    "\n",
    "print('Precisão: ', prec)\n",
    "print('Revocação: ', rev)\n",
    "\n",
    "print('\\nDiferença:', prec - rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercício 2: Expansão de consultas\n",
    "Crie grupos de equivalência para alguns termos de busca e calcule a diferença em termos de revocação e, possivelmente precisão, na resposta a consultas expandidas e não expandidas. Dica: use tempos verbais, pluralização, sinônimos, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = ['impressed', 'impress', 'impression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos encontrados (s/ stemming):\n",
      "\t- impressed: 17\n",
      "\t- impress: 1\n",
      "\t- impression: 2\n",
      "TOTAL: 20\n"
     ]
    }
   ],
   "source": [
    "# procurando termos equivalentes (sem stemming)\n",
    "res1 = set()\n",
    "\n",
    "print('Documentos encontrados (s/ stemming):')\n",
    "for word in group:\n",
    "    res = search(word, indice_token)\n",
    "    res1.update(res)\n",
    "    print('\\t- {}: {}'.format(word, len(res)))\n",
    "print('TOTAL: {}'.format(len(res1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos encontrados (com stemming):\n",
      "\t- impressed: 27\n",
      "\t- impress: 27\n",
      "\t- impression: 27\n",
      "TOTAL: 27\n"
     ]
    }
   ],
   "source": [
    "# procurando termos equivalentes (com stemming)\n",
    "res2 = set()\n",
    "\n",
    "print('Documentos encontrados (com stemming):')\n",
    "for word in group:\n",
    "    res = search(Stemmer.stem(word), indice_stem)\n",
    "    res2.update(res)\n",
    "    print('\\t- {}: {}'.format(word, len(res)))\n",
    "print('TOTAL: {}'.format(len(res2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisão:  1.0\n",
      "Revocação:  0.7407407407407407\n",
      "\n",
      "Diferença: 0.2592592592592593\n"
     ]
    }
   ],
   "source": [
    "print('Precisão: ', precisao(res1, res2))\n",
    "print('Revocação: ', revocacao(res1, res2))\n",
    "\n",
    "print('\\nDiferença:', precisao(res1, res2) - revocacao(res1, res2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercício 3: Verificação ortográfica\n",
    "\n",
    "Implemente uma expansão de consulta por meio da correção ortográfica. Utilize o corretor ortográfico [Pyenchant](http://pythonhosted.org/pyenchant/) para fazer as correções.\n",
    "\n",
    "\n",
    "<font color='red'>**Solução:**</font>\n",
    "\n",
    "- Link para o [repositório do PyEnchant](https://github.com/rfk/pyenchant/blob/master/website/content/tutorial.rst)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca PyEnchant\n",
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicionario\n",
    "dic = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conforme solicitado no exercício, vamos criar uma função de busca \"expandida\" que, caso a consulta esteja errada, ela sugere novas palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word(word):\n",
    "    if not dic.check(word):\n",
    "        print('Em vez disso, experimente pesquisar por: ')  # parecido com a sugestão do Google\n",
    "        for w in dic.suggest(word)[:3]:\n",
    "            print('  -', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em vez disso, experimente pesquisar por: \n",
      "  - problem\n"
     ]
    }
   ],
   "source": [
    "check_word('plobrem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_search(word, index, dic):\n",
    "    # faz a correção ortográfica\n",
    "    if not dic.check(word):\n",
    "        res = set()\n",
    "        for token in dic.suggest(word):\n",
    "            res = res | index[token]\n",
    "    else:\n",
    "        res = index[word]    \n",
    "    return res\n",
    "\n",
    "def extSearch(query, index, kind_index = 'token'):\n",
    "    # define o dicionário da língua\n",
    "    dic = enchant.Dict('en_US')\n",
    "\n",
    "    # tokenizando a busca\n",
    "    tokens = WPT().tokenize(query.lower())\n",
    "    \n",
    "    if kind_index == 'stem':\n",
    "        tokens = [Stemmer.stem(w) for w in tokens]\n",
    "\n",
    "    # criando um conjunto para armazenar o resultado (conj. para eliminar duplicados)\n",
    "    result = set()\n",
    "    \n",
    "    # definindo o tipo de operador (or, and or not)\n",
    "    operator = 'and'\n",
    "    if 'and' in tokens:\n",
    "        operator = 'and'\n",
    "    elif 'or' in tokens:\n",
    "        operator = 'or'\n",
    "    elif 'not' in tokens:\n",
    "        operator = 'not'\n",
    "    \n",
    "    # faz a consulta (query)\n",
    "    for token in tokens:\n",
    "        if token == 'and' or token == 'or' or token == 'not':\n",
    "            continue\n",
    "            \n",
    "        temp = correct_search(token, index, dic)\n",
    "        \n",
    "        if operator == 'or':\n",
    "            result = result | temp\n",
    "        elif operator == 'not':\n",
    "            result = result - temp\n",
    "        elif operator == 'and':\n",
    "            result = result | temp\n",
    "        else:\n",
    "            print('operador não definido')\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Agora, vamos fazer um teste comparando a função de busca do **Exercício 1** com a função de busca extendida que acabamos de criar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de busca \"crua\" (naïve)\n",
      "\t- Nº de resultados (não stemizados): 0\n",
      "\t- Nº de resultados (stemizados): 0\n",
      "\n",
      "\n",
      "Em vez disso, experimente pesquisar por: \n",
      "  - awful\n",
      "  - aw fur\n",
      "  - aw-fur\n",
      "\n",
      "Resultados de busca com correção ortográfica\n",
      "\t- Nº de resultados (não stemizados): 9\n",
      "\t- Nº de resultados (stemizados): 0\n"
     ]
    }
   ],
   "source": [
    "p = 'awfur'\n",
    "\n",
    "print('Resultados de busca \"crua\" (naïve)')\n",
    "print('\\t- Nº de resultados (não stemizados):', len(search(p, indice_token)))\n",
    "print('\\t- Nº de resultados (stemizados):', len(search(p, indice_stem, kind_index='stem')))\n",
    "\n",
    "print('\\n')\n",
    "check_word(p)\n",
    "\n",
    "print('\\nResultados de busca com correção ortográfica')\n",
    "print('\\t- Nº de resultados (não stemizados):', len(extSearch(p, indice_token)))\n",
    "print('\\t- Nº de resultados (stemizados):', len(extSearch(p, indice_stem, kind_index='stem')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercício 4: Consultas por frases\n",
    "Implemente um indice invertido que permita consulta por frases, conforme definido na aula 2.\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='red'>**Solução:**</font>\n",
    "\n",
    "Vamos implementar uma função de busca para fazer consultas por frase.\n",
    "\n",
    "**1. Primeiramente, vamos criar um novo índice para fazermos a busca por frase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as TWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"limpando\" os reviews (removendo stopwords, tokenizando e stemizando)\n",
    "def clean_text(text):\n",
    "    cleanText = [Stemmer.stem(token) for token in WPT().tokenize(str(text).lower()) if token not in swu]\n",
    "    cleanText = TWD().detokenize(cleanText)\n",
    "    \n",
    "    return cleanText\n",
    "\n",
    "# \"limpando todos os reviews\"\n",
    "corpus['clean_review'] = corpus['reviewText'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o índice de frases\n",
    "indice_frase = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for text_id, text in enumerate(corpus['clean_review'].apply(lambda x: WPT().tokenize(x))):\n",
    "    for token_id, token in enumerate(text):\n",
    "        indice_frase[token][text_id].append(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'item describ origin descript work without issu seen good product'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['clean_review'].iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([5, 10, 72, 138, 146, 246, 273, 417, 503, 510, 584, 595, 640, 647, 700, 731, 841, 862, 887, 947, 955, 959])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testando o índice\n",
    "word = 'described'\n",
    "\n",
    "indice_frase[Stemmer.stem(word)].keys() # retorna documentos que contém a palavra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Criando uma nova função de busca...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_proximity(list1, list2, distance):\n",
    "    '''\n",
    "    Compara a proximidade entre palavras em duas listas. \n",
    "    \n",
    "    Retorna a média da distância entre elas quando estas estão próximas (mais próximas que a \n",
    "    distância passada)\n",
    "    '''\n",
    "    res = []\n",
    "    \n",
    "    for w1 in list1:\n",
    "        for w2 in list2:\n",
    "            if abs(w1-w2) <= distance:\n",
    "                res.append(round((w1+w2)/2))\n",
    "                \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_phrase(phrase, index):\n",
    "    '''\n",
    "    Retorna o índice do documento que contém a frase buscada.\n",
    "    '''\n",
    "    tokens = [Stemmer.stem(token.lower()) for token in WPT().tokenize(phrase) if token not in swu]\n",
    "     \n",
    "    # armazena os elementos encontrados \n",
    "    matches = defaultdict(lambda:defaultdict(lambda:set([])))\n",
    "    for token in tokens:\n",
    "        matches[token] = index[token]\n",
    "    \n",
    "    # armazena os reviews encontrados (\"documento\")\n",
    "    documents = set(matches[tokens[0]])\n",
    "    for token in tokens:\n",
    "        documents = documents.intersection(set(matches[token]))\n",
    "    \n",
    "    res = set([]) \n",
    "    for doc in documents:\n",
    "        for pos in matches[tokens[0]][doc]:\n",
    "            if all((pos + token_id + 1 in matches[token][doc]) for token_id, token in enumerate(tokens[1:len(tokens)])):\n",
    "                res.add(doc) \n",
    "                \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This item is just as was described in the original description, works without any issues to be seen. Good product'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['reviewText'].iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5,\n",
       " 10,\n",
       " 72,\n",
       " 138,\n",
       " 146,\n",
       " 246,\n",
       " 273,\n",
       " 417,\n",
       " 503,\n",
       " 510,\n",
       " 584,\n",
       " 595,\n",
       " 640,\n",
       " 647,\n",
       " 700,\n",
       " 731,\n",
       " 841,\n",
       " 862,\n",
       " 887,\n",
       " 947,\n",
       " 955,\n",
       " 959}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_phrase(\"just as was described\", indice_frase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercício 5: Consulta híbrida.\n",
    "\n",
    "Modifique a solução acima para permitir respostas alternativas caso a frase não retorne resultados. Por exemplo, retornar, documentos que contenham parte da frase, ou uma busca booleana simples combinando as palavras da frase.\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='red'>**Solução:**</font>\n",
    "\n",
    "Para resolver esse exercício, vamos utilizar a biblioteca [whoosh](https://whoosh.readthedocs.io/en/latest/intro.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using whoosh version 2.7.4\n"
     ]
    }
   ],
   "source": [
    "import whoosh\n",
    "from whoosh.fields import *\n",
    "from whoosh.qparser import QueryParser\n",
    "from whoosh.index import create_in, open_dir\n",
    "\n",
    "print('Using whoosh version', whoosh.versionstring())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esse código foi escrito de acordo com o tutorial na página do Whoosh\n",
    "schema = Schema(content=TEXT(phrase=True, stored=True))\n",
    "\n",
    "if os.path.exists('indexdir'):\n",
    "    ix = open_dir('indexdir')\n",
    "else:\n",
    "    os.mkdir('indexdir')\n",
    "    ix = create_in(\"indexdir\", schema)\n",
    "    writer = ix.writer()\n",
    "    for txt in corpus['reviewText']:\n",
    "        writer.add_document(content=txt)\n",
    "    writer.commit()\n",
    "\n",
    "\n",
    "searcher = ix.searcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_search_phrase(phrase):\n",
    "        \n",
    "    tokens = [Stemmer.stem(token.lower()) for token in WPT().tokenize(phrase) if token not in swu]\n",
    " \n",
    "    res = search_phrase(phrase, indice_frase)\n",
    "    \n",
    "    # se não encontrar a frase, faz a busca pelo whoosh (retorna o resultado mais próximo)\n",
    "    if res == set([]):\n",
    "        searcher = ix.searcher()\n",
    "        query = QueryParser(\"content\", ix.schema).parse(phrase)\n",
    "        new_res = searcher.search(query)\n",
    "        return new_res.docs()\n",
    "    else:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{101, 280, 351, 458, 596}"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define uma frase para busca\n",
    "frase = 'love this product'\n",
    "\n",
    "new_search_phrase(frase) # retorna quais documentos contém essa frase (ou algo semelhante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally \u001b[42;37mlove this product\u001b[m!! I bought two - one for living room, one for bedroom.  I decided to try my hand in the bedroom first (in case I got frustrated and didn't finish the project).I've gotten items needing to be assembled before with all the hardware in one bag -- not so with this product!!  I'm impressed! Every nut, bolt, screw, washer, etc. is clearly packaged (individually by size and labeled!)The Arm also comes assembled -- which I disassembled for easier handling (the wall plate is attached to the Arm with two Carriage Bolts / easy to remove and makes holding in place while bolting to the wall a breeze).Took me 1-1/2 hours from start (gathering my tools) to finish (hanging the TV)  Oh, and I'm a 56 yr old disabled female.  I do have a  lot of construction/tool knowledge however.\n",
      "\n",
      "\n",
      "my girlfriend uses it every single day. i bought it for her as a christmas present. and she \u001b[41;38mlove\u001b[0ms it.\n",
      "\n",
      "\n",
      "It's a fine tablet.  The screen is excellent.  The speakers are very good.  The interface is a little clunky but easy to use.  It does just what I want it to do and it does it well.\n",
      "\n",
      "\n",
      "The Nook HD was designed to be a book reader, although it also is a great way to watch videos on Netflix, or listen to Pandora Radio, or to your favorite music. I added a 32gb memory card so that I can have thousands of my favorite songs with me when I travel.The Nook HD has a screen with the best resolution and the least glare and reflection of any color e-reader.  It is the only color e-reader that comes close to the reading experience of an e-ink reader.Barnes & Noble has lots of children's books that take advantage of the Nook's color and audio features.  I bought a Christmas story that can read out loud in English or Spanish, and on each page something would be moving...snow falling, or the moon moving across the sky.The Nook is VERY light, which makes it easy to read for hours, and you don't mind having your Nook with you 24/7 in your purse or \"man purse\" (I use a small laptop bag).The Barnes & Noble selection of e-books is huge, and includes 90% of best sellers.  Each day, there is a \"Book of the Day\" for $2 or $3...many of them best-sellers or classic books.\n",
      "\n",
      "\n",
      "First off, I want to start out by saying that I \u001b[41;38mlove\u001b[0m my Nook. My friend is an employee of B&N; so he was there to help guide me along in my selection and answer any questions I had. So, yes, I did have a little bias to the \u001b[41;38mproduct\u001b[0m to begin with. However, for what I need, this is perfect in almost every single way. In my review I'll compare the Nook to the Kindle ($139 version) and it's major differences, just so people know what I'm talking about in terms of other \u001b[41;38mproduct\u001b[0ms as we go down the list.The Bad (If you want to call it that):1. The Nook is a little heavier than the Kindle. I'm not sure of the exact weight differences, but just by feel alone it is a tad bit heavier. Now, that can be a good or bad thing depending on how you look at it. I for one don't find it to be a deal buster, the weight is not catastrophically heavier and uncomfortable, but it is heavier than the Kindle. I do think, however, that the durability of the Nook is a little better compared to the Kindle. The Nook has built in paddles to turn the pages, while the Kindle has flappy paddles along the side that feel as though they could break if I dropped it on it's side or if I touched it to hard. My suggestion for anyone interested in buying the Nook is to also buy a carrying case for it so it makes it easier to read, almost as if you were reading a real book. The covers at B&N; usually start around $30.2. The Nook does have a touch screen that has a bit of glare to it. Some others have posted their reviews of how they dislike the glare when the touch screen goes dark. For me it is a bit distracting if you find yourself wandering away from the text. However, whenever I read a book I find many distractions around me while I read. It's not a huge deal in anyway and some have posted if you turn it to the right angle you won't see your reflection. Again, not a deal buster in anyway I wouldn't think.3. The start up time is atrocious. I can't defend this one in anyway. Amish people can put up houses in a faster amount of time before the Nook would have a chance to get to your homepage. Ok, so not quite that slow, but you get the idea. I haven't timed it yet but I'm going to take an educated guess that it takes around 1-1/2 minutes to start up. That is something that should have been addressed from B&N; to begin with. Again, I can't defend this one in anyway.Ok, so what you have all been waiting for: The Good1. The extra storage feature is superb. The Nook comes with a built in 2GB storage space. Enough for most people, 2GB will get most every book you want to read in your Nook with no problem. However, if you are a fanatic reader that has Newspapers and Magazines delivered to your Nook everyday or every week, this can begin to take up space if you don't free it up. But don't fret! The Nook comes with an extra storage slot for your SD card that can hold up to whatever the highest SD card space is out there. The great thing about it is, SD cards are a lot cheaper today than you would have found 3 years ago, so adding space is cheap.2. The Nook has a featured called \"LendMe\". This is something that the Kindle does not have and I'm sure for competition sake will have in the future. This a great, great, great feature to the Nook! If you have any friends, family or co-workers that have a Nook and have a book on theirs that you would like to read, you can hook up wireless or through the computer and Lend them a copy of it for up to 14 days. If you're a fast reader then you can knock out a 500-600 page book in easily 14 days.3. Your county library might be on the list of Nook rentals. In the county in which I reside in North Carolina, our county library is fortunate enough to have a system in which I can rent books for 14 days from the library. This also is something the Kindle does not have. Before I go off on how great the feature for renting books is, I should point out that the library is going to have a limited selection of books for you to rent. I'm a big Thomas Sowell reader and he has approximately 30 books out on the market. However, the county library that I rent from has only 1 book from his selection. So do you're research before you buy a book, your county library might have it already. The feature, as I have seen, is relatively easy to use. You do have to hard wire your Nook to your computer after you download some software, but it's really simple and the kind people at your local county library can assist you on how to do this. In fact our county library had everything we needed in terms of software and directions on their website.4. Finally. You can take your Nook to B&N; and read any book for up to an hour. Recently I went to grab some coffee at B&N; and wanted to read a book on Ireland. Popped open my Nook, connected to the B&N; network (already programmed into your nook for ease), selected the book I wanted to read from the list I searched on, and wam-bam I was reading a book. You can read lots of books on one Nook from their online selection without having to lug 20 actual books that you wanted to compare before you bought.In closing on this review I would say first and foremost do your homework. Go to B&N; or BestBuy and try out the Kindle, Nook and even the Sony Reader. Some will really like the touch screen of the Nook compared to the touchpad of the Kindle, but it's all in what you like. Some will like the fact you can Lend books and some won't.I'm sure I've missed out on something that others would like to know about the \u001b[41;38mproduct\u001b[0m before buying, so if you have any questions please contact me back here on Amazon and I'll try to get to you as quickly as possible.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# conferindo se o resultado está correto\n",
    "for i in new_search_phrase(frase):\n",
    "    texto = corpus['reviewText'].iloc[i]\n",
    "    \n",
    "    if frase in texto:\n",
    "        # frase aparece exatamente igual\n",
    "        texto = texto.replace(frase, '\\033[42;37m{}\\033[m'.format(frase))\n",
    "        print(texto)\n",
    "    else:\n",
    "        # se a frase não está no texto, colore as palavras que aparecem   \n",
    "        for word in [Stemmer.stem(j) for j in WPT().tokenize(frase) if j not in swu]:\n",
    "            texto = texto.replace(word, '\\033[41;38m{}\\033[0m'.format(word))\n",
    "        print(texto)\n",
    "    \n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
